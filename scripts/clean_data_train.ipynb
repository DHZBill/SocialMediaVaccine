{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "STOP = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from gensim.utils import deaccent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH = '../data/vaccination_all_tweets.csv'\n",
    "GEO_PATH = '../data/country_vaccinations.csv'\n",
    "LABELED_PATH = '../data/covid_vaccine_tweets_with_sentiment.csv'\n",
    "\n",
    "TWEETS = pd.read_csv(TWEET_PATH)\n",
    "VACCINATION = pd.read_csv(GEO_PATH)\n",
    "LABELED = pd.read_csv(LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.360342e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>4,000 a day dying from the so called Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.382896e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Pranam message for today manifested in Dhyan b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.375673e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Hyderabad-based ?@BharatBiotech? has sought fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.381311e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Confirmation that Chinese #vaccines \"don�t hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362166e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Lab studies suggest #Pfizer, #Moderna vaccines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id  label                                         tweet_text\n",
       "0  1.360342e+18      1  4,000 a day dying from the so called Covid-19 ...\n",
       "1  1.382896e+18      2  Pranam message for today manifested in Dhyan b...\n",
       "2  1.375673e+18      2  Hyderabad-based ?@BharatBiotech? has sought fu...\n",
       "3  1.381311e+18      1  Confirmation that Chinese #vaccines \"don�t hav...\n",
       "4  1.362166e+18      3  Lab studies suggest #Pfizer, #Moderna vaccines..."
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for train dataset \n",
      "\n",
      "            Total\n",
      "tweet_id        0\n",
      "label           0\n",
      "tweet_text      0\n"
     ]
    }
   ],
   "source": [
    "def miss_val(df):\n",
    "    total=df.isnull().sum()\n",
    "    return pd.concat([total],axis=1,keys=['Total'])\n",
    "print(\"Missing values for train dataset \\n\")\n",
    "print(miss_val(LABELED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(string): \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",string)\n",
    "    return \" \".join(text.split())\n",
    "LABELED['tweet_text']=LABELED['tweet_text'].apply(lambda x:remove_link(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "    \n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    " \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "    \n",
    "## Export pickle\n",
    "def make_export(tr, tt, file_name):\n",
    "    train_export = train[['id']]\n",
    "    test_export = test[['id']]\n",
    "\n",
    "    try:\n",
    "        cur_shape = tr.shape[1]>1\n",
    "        train_export = pd.concat([train_export, tr], axis=1)\n",
    "        test_export = pd.concat([test_export, tt], axis=1)        \n",
    "    except:\n",
    "        train_export['p_comment'] = tr\n",
    "        test_export['p_comment'] = tt\n",
    "    \n",
    "    train_export.to_pickle(file_name + '_x_train.pkl')\n",
    "    test_export.to_pickle(file_name + '_x_test.pkl')\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\", \n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "        \n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "  \n",
    "def make_dict_cleaning(s, w_dict):\n",
    "    if check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break    \n",
    "## ----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1. Load Data\n",
      "1.2. Basic helpers\n"
     ]
    }
   ],
   "source": [
    "########################### Initial vars\n",
    "#################################################################################\n",
    "HELPER_PATH             = '../helper/'\n",
    "\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything\n",
    "\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "\n",
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('1.1. Load Data')\n",
    "good_cols       = ['tweet_id', 'tweet_text']\n",
    "if LOCAL_TEST:\n",
    "    tt          = pd.read_csv('../data/covid_vaccine_tweets_with_sentiment.csv', nrows=200000)\n",
    "    train       = tt.iloc[:-100000,:]\n",
    "    test        = tt.iloc[-100000:,:]\n",
    "    del tt\n",
    "    train, test = train[good_cols+['label']], test[good_cols]\n",
    "else:\n",
    "    train       = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "    test        = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')    \n",
    "    train, test = train[good_cols+['label', 'created_date']], test[good_cols]\n",
    "\n",
    "########################### Get basic helpers\n",
    "#################################################################################\n",
    "print('1.2. Basic helpers')\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "#good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "#bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "#global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "#global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "toxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 23251 | Known words: 5358\n"
     ]
    }
   ],
   "source": [
    "tweets = LABELED['tweet_text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "verbose = True\n",
    "global_lower=True\n",
    "tweets = tweets.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 18896 | Known words: 6398\n"
     ]
    }
   ],
   "source": [
    "if global_lower:\n",
    "    tweets = tweets.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 18889 | Known words: 6398\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "tweets = tweets.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 18889 | Known words: 6398\n"
     ]
    }
   ],
   "source": [
    "# Remove 'control' chars\n",
    "# Global    \n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 18889 | Known words: 6398\n"
     ]
    }
   ],
   "source": [
    "# Remove hrefs\n",
    "# Global    \n",
    "tweets = tweets.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 18616 | Known words: 6417\n",
      "�\n",
      "65533 --- \n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 18616 | Known words: 6417\n",
      "·\n",
      "183 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 18616 | Known words: 6417\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 18616 | Known words: 6417\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it)) \n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "    \n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "            \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 18616 | Known words: 6417\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break            \n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "        \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 18616 | Known words: 6417\n",
      ":-) --- 😁\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 18616 | Known words: 6417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 17843 | Known words: 6463\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(tweets, local_vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 17842 | Known words: 6463\n",
      "_? --- ?\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)       \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 17840 | Known words: 6463\n",
      "*** ---  *   *   * \n",
      "$$$ ---  $   $   $ \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 17839 | Known words: 6463\n",
      ":) --- 😁\n",
      ":( --- 😡\n",
      ";) --- 😜\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 17070 | Known words: 6506\n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 16918 | Known words: 6525\n",
      "10/10 --- 10 / 10\n",
      "w/no --- w / no\n",
      "15000/-for --- 15000 / -for\n",
      "gov/icmr --- gov / icmr\n",
      "b/w --- b / w\n",
      "w/o --- w / o\n",
      "13/04/2021 --- 13 / 04 / 2021\n",
      "clinics/sites --- clinics / sites\n",
      "p/b, --- p / b,\n",
      "covid-19/ --- covid-19 / \n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 16922 | Known words: 6530\n",
      "#students_against_covid --- #students against covid\n",
      "race/gender/faith,why --- race / gender / faith,why\n",
      "challenging/difficult/stressful --- challenging / difficult / stressful\n",
      "#covaxin/#covishield. --- #covaxin / #covishield.\n",
      "update/wibble/waffle. --- update / wibble / waffle.\n",
      ".@maryam_rajavi:khameneis --- .@maryam rajavi:khameneis\n",
      "1300gmt/1400cet/0800est --- 1300gmt / 1400cet / 0800est\n",
      "#tika_vaccination_utsav --- #tika vaccination utsav\n",
      "#perfomance_enhancing --- #perfomance enhancing\n",
      "crazy-polar-vortex-winter-storm --- crazy polar vortex winter storm\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 16581 | Known words: 6530\n",
      "#ahp --- word_placeholder[#___ahp]\n",
      "@ravishndtv --- word_placeholder[@___ravishndtv]\n",
      "#chennai --- word_placeholder[#___chennai]\n",
      "@99freemind --- word_placeholder[@___99freemind]\n",
      "#aot139spoilers --- word_placeholder[#___aot139spoilers]\n",
      "@sumanthraman --- word_placeholder[@___sumanthraman]\n",
      "@kelleypersonal --- word_placeholder[@___kelleypersonal]\n",
      "@ndtvfeed --- word_placeholder[@___ndtvfeed]\n",
      "@novy62 --- word_placeholder[@___novy62]\n",
      "#ino --- word_placeholder[#___ino]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags (add username/hashtag word?????)\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n",
    "        if word[len(word)-1].isalnum():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n",
    "        else:\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n",
    "\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 16580 | Known words: 6531\n",
      "@_5andman_ --- @_5andman\n",
      "@ian_hamilton_ --- @ian_hamilton\n",
      "@danil_bochkov_ --- @danil_bochkov\n",
      "_with_ --- _with\n",
      "cubs___ --- cubs\n",
      "@_sjpeace_ --- @_sjpeace\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 16580 | Known words: 6531\n",
      "_with --- with\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore \n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "data = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 12190 | Known words: 7022\n",
      "correct! --- correct !\n",
      "st. --- st .\n",
      "understand. --- understand .\n",
      "underwear, --- underwear ,\n",
      "structure. --- structure .\n",
      "imo. --- imo .\n",
      "kiya? --- kiya ?\n",
      "info. --- info .\n",
      "jan.1, --- jan.1 ,\n",
      "vintage! --- vintage !\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 11828 | Known words: 7049\n",
      "##moderna --- ## moderna\n",
      "-update --- - update\n",
      "?#vaccinated --- ?# vaccinated\n",
      "'meanwhile --- ' meanwhile\n",
      "-25c --- - 25c\n",
      "?@joebiden --- ?@ joebiden\n",
      "?10 --- ? 10\n",
      "@christina_mitas --- @ christina_mitas\n",
      "@cmo_england --- @ cmo_england\n",
      "#nation_with_modi --- # nation_with_modi\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 11828 | Known words: 7049\n",
      "f.d.a --- word_placeholder[fda]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else: \n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 11826 | Known words: 7049\n"
     ]
    }
   ],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 11824 | Known words: 7049\n",
      "-- --- -\n",
      "death--see --- death-see\n",
      "---- --- -\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 10749 | Known words: 7298\n",
      "#' ---  #  ' \n",
      "75-year-old --- 75 - year - old\n",
      "cold-storage --- cold - storage\n",
      "pre-market --- pre - market\n",
      "midday,it --- midday , it\n",
      "best-in-class --- best - in - class\n",
      "25.950 --- 25 . 950\n",
      "pdsa_itp --- pdsa _ itp\n",
      "abhina_prakash --- abhina _ prakash\n",
      "rich_pratt --- rich _ pratt\n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 10745 | Known words: 7300\n",
      "01l --- oil\n",
      "on3 --- one\n",
      "1bn --- ibn\n",
      "1ra --- ira\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion \n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "            \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word): \n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 9326 | Known words: 7524\n"
     ]
    }
   ],
   "source": [
    "# Open Holded words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 9110 | Known words: 7606\n",
      "companys --- company\n",
      "paras --- para\n",
      "yesterdays --- yesterday\n",
      "pathogens --- pathogen\n",
      "fevers --- fever\n",
      "trumps --- trump\n",
      "louies --- louie\n",
      "persists --- persist\n",
      "filipinos --- filipino\n",
      "heres --- here\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert emoji to text:\n",
      "Unknown words: 9110 | Known words: 7606\n"
     ]
    }
   ],
   "source": [
    "# Convert emoji to text\n",
    "# Local \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k in emoji.UNICODE_EMOJI)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.compile('[:_]').sub(' ', emoji.UNICODE_EMOJI.get(word)) \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert emoji to text:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4 , 000 a day dying from the so called covid -...\n",
       "1    pranam message for today manifested in dhyan b...\n",
       "2    hyderabad - based ? @ bharatbiotech ? has soug...\n",
       "3    confirmation that chinese # vaccines \" dont ha...\n",
       "4    lab studies suggest # pfizer , # moderna vacci...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.360342e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>4,000 a day dying from the so called Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.382896e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Pranam message for today manifested in Dhyan b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.375673e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Hyderabad-based ?@BharatBiotech? has sought fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.381311e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Confirmation that Chinese #vaccines \"don�t hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362166e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Lab studies suggest #Pfizer, #Moderna vaccines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.351285e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Still want to take the #jab? #PfizerBioNTech #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.377333e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>This time, Aerol�neas flight AR1068 goes to Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.363344e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>#Covaxin effective against mutant virus strain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.372580e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Safe and effective. #OxfordAstraZeneca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.367507e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>The day after the #Moderna #COVID19Vaccine... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id  label                                         tweet_text\n",
       "0  1.360342e+18      1  4,000 a day dying from the so called Covid-19 ...\n",
       "1  1.382896e+18      2  Pranam message for today manifested in Dhyan b...\n",
       "2  1.375673e+18      2  Hyderabad-based ?@BharatBiotech? has sought fu...\n",
       "3  1.381311e+18      1  Confirmation that Chinese #vaccines \"don�t hav...\n",
       "4  1.362166e+18      3  Lab studies suggest #Pfizer, #Moderna vaccines...\n",
       "5  1.351285e+18      1  Still want to take the #jab? #PfizerBioNTech #...\n",
       "6  1.377333e+18      2  This time, Aerol�neas flight AR1068 goes to Mo...\n",
       "7  1.363344e+18      3  #Covaxin effective against mutant virus strain...\n",
       "8  1.372580e+18      3             Safe and effective. #OxfordAstraZeneca\n",
       "9  1.367507e+18      2  The day after the #Moderna #COVID19Vaccine... ..."
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELED.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83927f8280031deb3aa1314dc0bf4bd4fab6545592770c6e05f6ffa72755e8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
